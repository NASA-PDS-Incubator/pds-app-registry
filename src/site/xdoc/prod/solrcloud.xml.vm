<?xml version="1.0" encoding="UTF-8"?>

<!--
  Copyright 2019, California Institute of Technology ("Caltech").
  U.S. Government sponsorship acknowledged.
  
  All rights reserved.
  
  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions are met:
  
  * Redistributions of source code must retain the above copyright notice,
  this list of conditions and the following disclaimer.
  * Redistributions must reproduce the above copyright notice, this list of
  conditions and the following disclaimer in the documentation and/or other
  materials provided with the distribution.
  * Neither the name of Caltech nor its operating division, the Jet Propulsion
  Laboratory, nor the names of its contributors may be used to endorse or
  promote products derived from this software without specific prior written
  permission.
  
  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
  POSSIBILITY OF SUCH DAMAGE.
-->

<document>
<properties>
  <title>SolrCloud</title>
</properties>

<body>

<section name="SolrCloud">

<p>
<ul>
  <li><a href="#Overview">Overview</a></li>
  <li><a href="#Deployment_Summary">Deployment Summary</a></li>
  <li><a href="#High_Availability_Summary">High Availability Summary</a></li>
  <li><a href="#Shards_and_Replicas">Shards and Replicas</a></li>
  <li><a href="#Cluster_Expansion">Cluster Expansion</a></li>
</ul>
</p>

<subsection name="Overview">

<p>
SolrCloud is a cluster of Solr servers that provides fault tolerance and high availability.
It uses Apache ZooKeeper for cluster configuration and coordination.
Number of nodes you need, usually depends on amount of data, number of queries, fault tolerance and high availability requirements.
</p>
<p>
There are two main SolrCloud deployment types: single Solr server with embedded ZooKeeper and
multinode deployment with separate Solr and ZooKeeper clusters.
</p>

<!-- ============================== -->

<h4>Single Solr Server with Embedded ZooKeeper</h4>
<p>
This is the simplest configuration of SolrCloud. A single instance of ZooKeeper and Solr server are running in the same JVM.
This setup does not provide any fault tolerance or high availability, and could not be expanded.
This configuration is recommended for development or testing and can be easily installed on a laptop or desktop.
</p>
<img src="../images/solr-cloud-dev.png" style="width:12em;" />

<!-- ============================== -->

<h4>Multinode Cluster</h4>
<p>
This configuration is recommended for production. It provides fault tolerance and high availability.
Both ZooKeeper and Solr clusters can be expanded by adding more nodes.
</p>
<img src="../images/solr-cloud-prod.png" style="width:28em;" />

</subsection>

<!-- ============================== -->

<subsection name="Deployment Summary">

<table>
<tr><th>Deployment type</th><td>Multinode cluster</td><td>Single Solr server with embedded ZooKeeper</td></tr>
<tr><th>Usage</th><td>Production / QA</td><td>Development / QA</td></tr>
<tr><th>Scalability</th><td>Yes</td><td>No</td></tr>
<tr><th>Fault tolerance and high availability</th><td>Yes</td><td>No</td></tr>
<tr><th>Deployment options</th><td>On-prem or cloud: VMs, Kubernetes</td><td>Laptop or desktop: host, VM, docker</td></tr>
</table>
</subsection>

<!-- ============================== -->

<subsection name="High Availability Summary">

<table>
<tr><th>Number of ZooKeeper nodes</th><th>Number of Solr nodes</th><th>Fault tolerance / High availability</th></tr>
<tr><td>1</td><td>1</td><td>No</td></tr>
<tr><td>1</td><td>2+</td><td>ZooKeeper - No, Solr - Yes</td></tr>
<tr><td>3</td><td>2+</td><td>Yes</td></tr>
</table>

</subsection>

<!-- ============================== -->

<subsection name="Cores, Shards and Replicas">

<ul>
<li>
A <b>core</b> is an instance of a Lucene index.
</li>
<li>
A <b>shard</b> is a logical partition of the collection, containing a subset of documents from the collection.
For example, if "registry" collection has 3,000 documents and 3 shards, then each shard has about 1,000 document.
Each shard has one or more replicas.
</li>
<li>
A <b>replica</b> is a copy of a shard. Each replica exists within Solr as a core.
For example, if "registry" collection has 3,000 documents, 3 shards and 2 replicas (per shard),
then there are 6 cores (3 shards x 2 replicas) containing 1,000 documents each. There are 2 copies of each document.
</li>
<li>
Every shard has a <b>leader</b> replica. Leaders are automatically elected.
If a leader goes down, one of the other replicas is automatically elected as the new leader.
</li>
<li>
When a document is sent to a Solr node for <b>indexing</b>, the system first determines which shard that document belongs to, 
and then which node is currently hosting the leader for that shard. 
The document is then forwarded to the current leader for indexing, and the leader forwards the update to all of the other replicas.
</li>
<li>
By default, all replicas (cores) serve <b>queries</b>.
</li>
</ul>

<p>
You can read more about shards and indexing data in SolrCloud at 
<a href="https://lucene.apache.org/solr/guide/8_6/shards-and-indexing-data-in-solrcloud.html" target="_blank">Solr web site</a>.
</p>

<h4>Examples</h4>

<p>
Below are few example configurations for a collection with different number of shards and replicas in 3 node Solr cluster.
The collection contains 3,000 documents.
</p>

<p>
<b>1 Shard, 1 Replica</b>
</p>
<img src="../images/shards-1.png" style="width:40em;" />

<p>
<ul>
  <li>Concurrent writes - 1 node</li>
  <li>Concurrent reads - 1 node</li>
  <li>Fault tolerant - No</li>
</ul>
</p>

<p>
<b>1 Shard, 3 Replicas</b>
</p>
<img src="../images/shards-2.png" style="width:40em;" />

<p>
<ul>
  <li>Concurrent writes - 1 node</li>
  <li>Concurrent reads - 3 nodes</li>
  <li>Fault tolerant - Yes (2 nodes could go down)</li>
</ul>
</p>

<p>
<b>3 Shards, 2 Replicas</b>
</p>
<img src="../images/shards-3.png" style="width:40em;" />

<p>
<ul>
  <li>Concurrent writes - 3 nodes</li>
  <li>Concurrent reads - 3 nodes (6 partitions)</li>
  <li>Fault tolerant - Yes (1 node could go down)</li>
</ul>
</p>

<h4>Creating Collections</h4>

<p>
By default, when you create a collection in Solr, the number of shards and replicas is 1.
The following example shows how to create registry collection with 3 shards and 2 replicas.
</p>

<source>
registry-manager create-registry -zkHost zk1.test.local -shards 3 -replicas 2
</source>

</subsection>

<!-- ====================================================== -->

<subsection name="Cluster Expansion">
<h4>Getting Information about Solr Collections</h4>
<p>
Before expanding the cluster you may need to get more information about your Solr collection(s).
The following example shows how to call Solr API to get information about "registry" collection.
</p>
<source>
curl "http://solr1.test.local:8983/api/c/registry"
</source>

<p>
The response is a long JSON, similar to this
</p>
<source>
{
  "responseHeader":{
    "status":0,
    "QTime":26},
  "cluster":{
    "collections":{
      "registry":{
        "pullReplicas":"0",
        "replicationFactor":"1",
        "shards":{"shard1":{
            "range":"80000000-7fffffff",
            "state":"active",
            "replicas":{"core_node2":{
                "core":"registry_shard1_replica_n1",
                "base_url":"http://solr2.test.local:8983/solr",
                "node_name":"solr2.test.local:8983_solr",
                "state":"active",
                "type":"NRT",
                "force_set_state":"false",
                "leader":"true"}}}},
        "router":{"name":"compositeId"},
        "maxShardsPerNode":"1",
        "autoAddReplicas":"false",
        "nrtReplicas":"1",
        "tlogReplicas":"0",
        "znodeVersion":4,
        "configName":"registry"}},
    "live_nodes":["solr2.test.local:8983_solr",
      "solr1.test.local:8983_solr"]}}
</source>

<p>
As you can see, collection "registry", has only one shard called "shard1". The shard has one replica called "core_node2"
located on node "solr2.test.local:8983_solr".
</p>

<h4>Adding Replicas to a Shard</h4>
<p>
To add a replica to "shard1" of collection "registry" call one of the following Solr APIs.
</p>

<source>
# API v1
curl "http://localhost:8983/solr/admin/collections?action=ADDREPLICA&amp;collection=registry&amp;shard=shard1"

# API v2
curl "http://solr1.test.local:8983/api/c/registry/shards" \
  -XPOST -H 'Content-type:application/json' \
  -d '{ add-replica: { shard: "shard1" } }'
</source>

<p>
Get "registry" collection info and check that new replica was created.
</p>

<source>
curl "http://solr1.test.local:8983/api/c/registry
</source>

<p>
In our example, "shard1" now has two replicas "core_node2" and "core_node4".
Solr naming convention for replicas, shards and cores is strange. You can see some random numbers in names.
</p>

<source>
...
"shards":{"shard1":{
    ...
    "replicas":{
      "core_node2":{
        "core":"registry_shard1_replica_n1",
        "node_name":"solr2.test.local:8983_solr",
        ...
        "state":"active",
        "leader":"true"},
      "core_node4":{
        "core":"registry_shard1_replica_n3",
        "node_name":"solr1.test.local:8983_solr",
        ...
        "state":"active" }}}},
</source>

<p>
Note that Solr placed each replica on a different node. In some cases it might not work as expected.
There is an API to move a replica from one node to another. In the following example, replica "core_node4"
is moved from node "solr2.test.local:8983_solr" to "solr3.test.local:8983_solr".
The whole command should be on a single line. We split it here to make it more readable.
</p>

<source>
curl "http://localhost:8983/solr/admin/collections?action=MOVEREPLICA
  &amp;collection=registry
  &amp;shard=shard1
  &amp;replica=core_node4
  &amp;sourceNode=solr2.test.local:8983_solr
  &amp;targetNode=solr3.test.local:8983_solr"
</source>

<p>
You can learn more about replica management APIs at
<a href="https://lucene.apache.org/solr/guide/8_6/replica-management.html" target="_blank">Solr website</a>.
Also see <a href="https://lucene.apache.org/solr/guide/8_6/v2-api.html">v2 API docs</a>.
</p>

<p>
You can also view some partial information about collections and its replicas in Solr admin UI.
</p>
<p>
<img src="../images/solr-ui-nodes.png" style="width:80em;" />
</p>


<!-- ======================= -->

<h4>Increasing Number of Shards</h4>

<p>
There is a Solr API to split a shard into 2 or more pieces. 
In the following example, we will split "shard1" of "registry" collection into 2 new shards.
</p>
<source>
curl "http://localhost:8983/solr/admin/collections?action=SPLITSHARD&amp;collection=registry&amp;shard=shard1"
</source>

<p>
Old shard, "shard1", will be deactivated and two new shards, "shard1_0" and "shard1_1" will be created.
"Shard1" had 2 replicas, so both new shards, "shard1_0" and "shard1_1", will have 2 replicas too.
</p>

<p>
For some reason, Solr placed both replicas of "shard1_1" on the same node, so I had to move one of the replicas
to another node.
</p>

<source>
curl "http://localhost:8983/solr/admin/collections?action=MOVEREPLICA
  &amp;collection=registry
  &amp;shard=shard1_1
  &amp;replica=core_node10
  &amp;sourceNode=solr1.test.local:8983_solr
  &amp;targetNode=solr2.test.local:8983_solr"
</source>

<p>
Now old shard can be deleted.
</p>
<source>
curl "http://localhost:8983/solr/admin/collections?action=DELETESHARD&amp;collection=registry&amp;shard=shard1"
</source>

<p>
You can learn more about shard management APIs at
<a href="https://lucene.apache.org/solr/guide/8_6/shard-management.html" target="_blank">Solr website</a>.
Also see <a href="https://lucene.apache.org/solr/guide/8_6/v2-api.html">v2 API docs</a>.
</p>

</subsection>

</section>

</body>

</document>

